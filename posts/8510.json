{"bookmarked": 10, "bucket_name": "Today", "bucket_order": 3, "change_log": [{"anon": "stud", "data": "jqgjwhs48zy1tz", "type": "create", "uid_a": "a_0", "when": "2019-01-03T11:52:26Z"}, {"anon": "stud", "to": "jqgjwhrzd6r1ty", "type": "followup", "uid_a": "a_1", "when": "2019-01-03T14:05:41Z"}, {"anon": "stud", "to": "jqgjwhrzd6r1ty", "type": "followup", "uid_a": "a_2", "when": "2019-01-03T14:58:09Z"}, {"anon": "no", "to": "jqgjwhrzd6r1ty", "type": "followup", "uid": "idrilg16trt4ij", "when": "2019-01-04T18:17:35Z"}, {"anon": "no", "data": "jqm2z4p5lf72ty", "to": "jqgjwhrzd6r1ty", "type": "s_answer", "uid": "hyw7jbf3qk2160", "when": "2019-01-07T08:45:12Z"}, {"anon": "no", "data": "jqm30v7eme54al", "type": "s_answer_update", "uid": "hyw7jbf3qk2160", "when": "2019-01-07T08:46:33Z"}, {"anon": "no", "data": "jqm33l8bt5z53g", "type": "s_answer_update", "uid": "hyw7jbf3qk2160", "when": "2019-01-07T08:48:40Z"}, {"anon": "no", "data": "jqm34c1e8135dy", "type": "s_answer_update", "uid": "hyw7jbf3qk2160", "when": "2019-01-07T08:49:15Z"}, {"anon": "no", "data": "jqm3dll4w8s5y", "type": "s_answer_update", "uid": "hyw7jbf3qk2160", "when": "2019-01-07T08:56:27Z"}, {"anon": "no", "data": "jqm3gv0urhw164", "type": "s_answer_update", "uid": "hyw7jbf3qk2160", "when": "2019-01-07T08:58:59Z"}, {"anon": "no", "data": "jqm3nywqz506sl", "type": "s_answer_update", "uid": "hyw7jbf3qk2160", "when": "2019-01-07T09:04:31Z"}, {"anon": "stud", "to": "jqgjwhrzd6r1ty", "type": "followup", "uid_a": "a_0", "when": "2019-01-07T09:07:14Z"}, {"anon": "no", "to": "jqgjwhrzd6r1ty", "type": "feedback", "uid": "hyw7jbf3qk2160", "when": "2019-01-07T09:10:05Z"}, {"anon": "stud", "to": "jqgjwhrzd6r1ty", "type": "feedback", "uid_a": "a_0", "when": "2019-01-07T09:17:07Z"}, {"anon": "no", "to": "jqgjwhrzd6r1ty", "type": "feedback", "uid": "hyw7jbf3qk2160", "when": "2019-01-07T09:48:51Z"}, {"anon": "stud", "to": "jqgjwhrzd6r1ty", "type": "feedback", "uid_a": "a_0", "when": "2019-03-13T19:58:32Z"}], "children": [{"anon": "stud", "bucket_name": "Week 12/30 - 1/5", "bucket_order": 64, "children": [], "config": {}, "created": "2019-01-03T14:05:41Z", "data": {"embed_links": null}, "folders": [], "id": "jqgonuvwu8x796", "no_answer": 0, "no_upvotes": 0, "subject": "127 is decent", "tag_good": [], "tag_good_arr": [], "type": "followup", "uid_a": "a_1", "updated": "2019-01-03T14:05:41Z"}, {"anon": "stud", "bucket_name": "Week 12/30 - 1/5", "bucket_order": 64, "children": [], "config": {}, "created": "2019-01-03T14:58:09Z", "data": {"embed_links": null}, "folders": [], "id": "jqgqjcb8o195wj", "no_answer": 0, "no_upvotes": 0, "subject": "I haven&#39;t read the book in its entirety, but the first few chapters so here&#39;s my opinion for what its worth). I think there are two good ways to get the foundational material to read that book. The first is through CS189 (although more in the style of how Professor Sahai taught it), check out Fa17/Sp18 course notes. The other way is to take Stat 135 (you&#39;ll get a lot of stuff thats not entirely useful like survey sampling and a random assortment of tests), but then after the course read through the final chapter of the textbook (Rice Mathematical Statistics I think) on linear regression. Stat 135 covers a bit of it, but not enough to give the background to read ESL. After either choice, you&#39;d have enough background to understand most of ESL; there will still be a few linear algebra ideas (like certain matrix factorizations such as SVD) that you might be missing and EE127 would help. \n\nI&#39;d recommend that if you choose the CS189 route (go through Sp18 course notes or something), find a pdf of the Stat 135 textbook and try and read the last chapter of it anyways since the notation they use is similar, and familiarize yourself so that when you are reading ESL you won&#39;t get hung over by the small stuff.", "tag_good": [], "tag_good_arr": [], "type": "followup", "uid_a": "a_2", "updated": "2019-01-03T14:58:09Z"}, {"anon": "no", "bucket_name": "Week 12/30 - 1/5", "bucket_order": 64, "children": [], "config": {}, "created": "2019-01-04T18:17:35Z", "data": {"embed_links": null}, "folders": [], "id": "jqid3nf22u12u", "no_answer": 0, "no_upvotes": 0, "subject": "<p>I&#39;ve also started reading through\u00a0ESL myself this break! It seems like a really cool book. I&#39;ve taken Data100, CS170, EE126, and EE127 (189 next semester), so I&#39;d say I&#39;m in a pretty similar boat and have been making decent progress (but also haven&#39;t delved too deep yet).\u00a0</p>\n<p></p>\n<p>My advice would definitely be to take 127. If you&#39;ve &#34;heard that it&#39;s not the greatest&#34;, it&#39;s probably\u00a0due to\u00a0the course organization\u2013I admit the course is not as well organized as other EECS classes and the staff can sometimes be hard to reach. On the other hand, it&#39;s not quite as much work as 126 or 170 (bi-weekly psets).\u00a0Despite all this, the course material is fascinating and invaluable if you want to go the ML theory route, you can cast almost anything in ML as an optimization problem. Math 54 is just not enough and from what I&#39;ve heard Math 110 doesn&#39;t really teach you the same kind of practical linear algebra that you learn in 127. I have absolutely no regrets, optimization is\u00a0super interesting and El\u00a0Ghaoui\u00a0is very knowledgeable.\u00a0</p>\n<p></p>\n<p>If you want to get a head start, I would recommend starting <a href=\"http://web.stanford.edu/~boyd/cvxbook/\" target=\"_blank\">Stephen Boyd&#39;s convex optimization book</a>, imo its the best resource. If you end up taking 127, I would also recommend following along in Boyd. You\u00a0can also read the first few chapters of the <a href=\"https://people.eecs.berkeley.edu/~elghaoui/optmodbook.html\" target=\"_blank\">official course textbook</a>\u00a0for the linear algebra chops. Cheers!</p>", "tag_good": [], "tag_good_arr": [], "type": "followup", "uid": "idrilg16trt4ij", "updated": "2019-01-04T18:17:35Z"}, {"bucket_name": "Today", "bucket_order": 3, "children": [], "config": {}, "created": "2019-01-07T08:45:12Z", "data": {"embed_links": []}, "folders": [], "history": [{"anon": "no", "content": "<p>Apologize in advance for the lengthy response:</p>\n<p></p>\n<p>ESL has some really frontier topics at the end. To understand probabilistic graphical models and the junction tree algorithm etc. ( I believe chapter 11), CS 281A would be most relevant since usually the focus of this class is PGMs, generalizing the Viterbi algorithm from 126. Disclaimer: I haven&#39;t taken 281A since it was never offered, just read a few of the notes. I heard the sequel 281B is focused on online learning and bandits, but I don&#39;t recall whether this material is in ESL.</p>\n<p></p>\n<p></p>\n<p>For the high dimensional statistics /concentration of measure material (chapter 12 I think) , this requires a high degree of mathematical maturity and knowledge of measure theoretic ideas like Radon-Nikodym derivatives. This is something that isn&#39;t really needed unless you plan on pursuing a PhD in ML, but you&#39;ll see some of the results come up in models seen often such as LASSO regression. The way to get yourself to understand this frontier topic would be to do something like take Math 104(real analysis), then 202A (measure theory) since this requires 104 level knowledge , and Stat 210A (mainly about parametric statistical models) followed by 210B (the high dimensional statistics/nonparametric statistics class here). I have not taken 210B, but have taken Math 104, 202A, and 210A, and I can answer any general questions about the material in these classes. The hypothesis testing/MLE in 210A may be a bit redundant with 126. Also , this is getting a bit far off but I believe Professor Michael Jordan (teaching 210B in Spring) recommends eventually studying functional analysis (Math 202B, although it was introduced at the end of our 202A class) for nonparametric statistics if you plan on doing really theoretical ML work. Personally I&#39;ve decided to put off 210B and 202B till graduate school but again this kind of depends on what area of statistical learning theory you want to study from ESL.</p>\n<p></p>\n<p>Also, it may seem slightly strange at first, but EE229A if you have sufficient preparation (sounds like you definitely do with 126) does cover concentration of measure material at the end from an information theoretic perspective. I have also taken this, along with Math 110, so feel free to ask about it as well. My opinion of Math 110 is that it&#39;s very useful for looking at linear algebra from an axiomatic point of view, and really understanding positive definite, semi definite forms etc. will help in the quadratic programming part of 127. For numerical linear algebra that&#39;s used in optimization though it does not cover much in terms of SVD etc. Math 221 or 128A covers this better (I only took 128A). Numerical analysis also introduces you to proving the convergence of iterative techniques such as Newton&#39;s method, which is really useful for numerical optimization, (accelerated gradient descent etc.), a big component of deep learning nowadays (also in ESL).</p>", "created": "2019-01-07T09:04:31Z", "subject": "", "uid": "hyw7jbf3qk2160"}, {"anon": "no", "content": "<p>Apologize in advance for the lengthy response:</p>\n<p></p>\n<p>ESL has some really frontier topics at the end. To understand probabilistic graphical models and the junction tree algorithm etc. ( I believe chapter 11), CS 281A would be most relevant since usually the focus of this class is PGMs, generalizing the Viterbi algorithm from 126. Disclaimer: I haven&#39;t taken 281A since it was never offered, just read a few of the notes. I heard the sequel 281B is focused on online learning and bandits, but I don&#39;t recall whether this material is in ESL.</p>\n<p></p>\n<p></p>\n<p>For the high dimensional statistics /concentration of measure material (chapter 12 I think) , this requires a high degree of mathematical maturity and knowledge of measure theoretic ideas like Radon-Nikodym derivatives. This is something that isn&#39;t really needed unless you plan on pursuing a PhD in ML, but you&#39;ll see some of the results come up in models seen often such as LASSO regression. The way to get yourself to understand this frontier topic would be to do something like take Math 104(real analysis), then 202A (measure theory) since this requires 104 level knowledge , and Stat 210A (mainly about parametric statistical models) followed by 210B (the high dimensional statistics/nonparametric statistics class here). I have not taken 210B, but have taken Math 104, 202A, and 210A, and I can answer any general questions about the material in these classes. The hypothesis testing/MLE in 210A may be a bit redundant with 126. Also , this is getting a bit far off but I believe Professor Michael Jordan (teaching 210B in Spring) recommends eventually studying functional analysis (Math 202B, although it was introduced at the end of our 202A class) for nonparametric statistics if you plan on doing really theoretical ML work. Personally I&#39;ve decided to put off 210B and 202B till graduate school but again this kind of depends on what area of statistical learning theory you want to study from ESL.</p>\n<p></p>\n<p>Also, it may seem slightly strange at first, but EE229A if you have sufficient preparation (sounds like you definitely do with 126) does cover concentration of measure material at the end from an information theoretic perspective. I have also taken this, along with Math 110, so feel free to ask about it as well. My opinion of Math 110 is that it&#39;s very useful for looking at linear algebra from an axiomatic point of view, and really understanding positive definite, semi definite forms etc. will help in the quadratic programming part of 127. For numerical linear algebra that&#39;s used in optimization though it does not cover much in terms of SVD etc. Math 221 or 128A covers this better (I only took the latter). Numerical analysis also introduces you to proving convergence of iterative techniques, which is really useful for numerical optimization, (accelerated gradient descent etc.), a big component of deep learning nowadays (also in ESL).</p>", "created": "2019-01-07T08:58:59Z", "subject": "", "uid": "hyw7jbf3qk2160"}, {"anon": "no", "content": "<p>Apologize in advance for the lengthy response:</p>\n<p></p>\n<p>ESLR has some really frontier topics at the end. To understand probabilistic graphical models and the junction tree algorithm etc. ( I believe chapter 11), CS 281A would be most relevant since usually the focus of this class is PGMs, generalizing the Viterbi algorithm from 126. Disclaimer: I haven&#39;t taken 281A since it was never offered, just read a few of the notes. I heard the sequel 281B is focused on online learning and bandits, but I don&#39;t recall whether this material is in ESLR.</p>\n<p></p>\n<p></p>\n<p>For the high dimensional statistics /concentration of measure material (chapter 12 I think) , this requires a high degree of mathematical maturity and knowledge of measure theoretic ideas like Radon-Nikodym derivatives. This is something that isn&#39;t really needed unless you plan on pursuing a Phd in ML, but you&#39;ll see some of the results come up in models seen often such as Lasso regression. The way to get yourself to understand this frontier topic would be to do something like take Math 104(real analysis), then 202A (measure theory) since this requires 104 level knowledge , and Stat 210A (mainly about parametric statistical models) followed by 210B (the high dimensional statistics/nonparametric statistics class here). I have not taken 210B, but have taken Math 104, 202A, and 210A, and I can answer any general questions about the material in these classes. The hypothesis testing/MLE in 210A may be a bit redundant with 126. Also , this is getting a bit far off but I believe Profe<a href=\"/class/hyq0br1u3kx7dg?cid=8510\">submit</a>ssor Michael Jordan (teaching 210B in Spring) recommends eventually studying functional analysis (Math 202B, although it was introduced at the end of our 202A class) for nonparametric statistics if you plan on doing really theoretical ML work. Personally I&#39;ve decided to put off 210B and 202B till graduate school but again this kind of depends on what area of statistical learning theory you want to study from ESLR.</p>\n<p></p>\n<p>Also, it may seem slightly strange at first, but EE229A if you have sufficient preparation (sounds like you definitely do with 126) does cover concentration of measure material at the end from an information theoretic perspective. I have also taken this, along with Math 110, so feel free to ask about it as well. My opinion of Math 110 is that it&#39;s very useful for looking at linear algebra from an axiomatic point of view, and really understanding positive definite, semi definite forms etc. will help in the quadratic programming part of 127. For numerical linear algebra that&#39;s used in optimization though it does not cover much in terms of SVD etc. Math 221 or 128A covers this better (I only took the latter). Numerical analysis also introduces you to proving convergence of iterative techniques, which is really useful for numerical optimization, (accelerated gradient descent etc.), a big component of deep learning nowadays (also in ESLR).</p>", "created": "2019-01-07T08:56:27Z", "subject": "", "uid": "hyw7jbf3qk2160"}, {"anon": "no", "content": "<p>Apologize in advance for the lengthy response:</p>\n<p>ESLR has some really frontier topics at the end, and to be fairly honest to understand probabilistic graphical models and the junction tree algorithm etc. ( I believe chapter 11), CS 281A (statistical learning theory) would be most relevant since usually the focus of this class is PGMs which generalize the Viterbi algorithm from 126. Disclaimer: I haven&#39;t taken 281A, just read a few of the notes. For the high dimensional statistics /concentration of measure material (chapter 12 I think) , this requires a high degree of mathematical maturity and knowledge of measure theoretic ideas like Radon-Nikodym derivatives. This is something that isn&#39;t really needed unless you plan on pursuing a Phd in ML, but you&#39;ll see some of the results come up in models seen often such as Lasso regression. The way to get yourself to understand this frontier topic would be to do something like take Math 104(real analysis), then 202A (measure theory) since this requires 104 level knowledge , and Stat 210A (mainly about parametric statistical models) followed by 210B (the high dimensional statistics/nonparametric statistics class here). I have not taken 210B, but have taken Math 104, 202A, and 210A, and I can answer any general questions about the material in these classes. The hypothesis testing/MLE in 210A may be a bit redundant with 126. Also , this is getting a bit far off but I believe Professor Michael Jordan (teaching 210B in Spring) recommends eventually studying functional analysis (Math 202B, although it was introduced at the end of our 202A class) for nonparametric statistics if you plan on doing really theoretical ML work. Personally I&#39;ve decided to put off 210B and 202B till graduate school but again this kind of depends on what area of statistical learning you want to study from ESLR. Also, it may seem slightly strange at first, but EE229A if you have sufficient preparation (sounds like you definitely do with 126) does cover concentration of measure material at the end from an information theoretic perspective. I have also taken this, along with Math 110, so feel free to ask about it as well. My opinion of Math 110 is that it&#39;s very useful for looking at linear algebra from an axiomatic point of view, and really understanding positive definite, semi definite forms etc. will help in the quadratic programming part of 127. For numerical linear algebra that&#39;s used in optimization though it does not cover much in terms of SVD etc. Math 221 or 128A covers this better (I only took the latter). Numerical analysis also introduces you to proving convergence of iterative techniques, which is really useful for numerical optimization, (accelerated gradient descent etc.), a big component of deep learning nowadays (also in ESLR).</p>", "created": "2019-01-07T08:49:15Z", "subject": "", "uid": "hyw7jbf3qk2160"}, {"anon": "no", "content": "<p>ESLR has some really frontier topics at the end, and to be fairly honest to understand probabilistic graphical models and the junction tree algorithm etc. ( I believe chapter 11), CS 281A (statistical learning theory) would be most relevant since usually the focus of this class is PGMs which generalize the Viterbi algorithm from 126. Disclaimer: I haven&#39;t taken 281A, just read a few of the notes. For the high dimensional statistics /concentration of measure material (chapter 12 I think) , this requires a high degree of mathematical maturity and knowledge of measure theoretic ideas like Radon-Nikodym derivatives. This is something that isn&#39;t really needed unless you plan on pursuing a Phd in ML, but you&#39;ll see some of the results come up in models seen often such as Lasso regression. The way to get yourself to understand this frontier topic would be to do something like take Math 104(real analysis), then 202A (measure theory) since this requires 104 level knowledge , and Stat 210A (mainly about parametric statistical models) followed by 210B (the high dimensional statistics/nonparametric statistics class here). I have not taken 210B, but have taken Math 104, 202A, and 210A, and I can answer any general questions about the material in these classes. The hypothesis testing/MLE in 210A may be a bit redundant with 126. Also , this is getting a bit far off but I believe Professor Michael Jordan (teaching 210B in Spring) recommends eventually studying functional analysis (Math 202B, although it was introduced at the end of our 202A class) for nonparametric statistics if you plan on doing really theoretical ML work. Personally I&#39;ve decided to put off 210B and 202B till graduate school but again this kind of depends on what area of statistical learning you want to study from ESLR. Also, it may seem slightly strange at first, but EE229A if you have sufficient preparation (sounds like you definitely do with 126) does cover concentration of measure material at the end from an information theoretic perspective. I have also taken this, along with Math 110, so feel free to ask about it as well. My opinion of Math 110 is that it&#39;s very useful for looking at linear algebra from an axiomatic point of view, and really understanding positive definite, semi definite forms etc. will help in the quadratic programming part of 127. For numerical linear algebra that&#39;s used in optimization though it does not cover much in terms of SVD etc. Math 221 or 128A covers this better (I only took the latter). Numerical analysis also introduces you to proving convergence of iterative techniques, which is really useful for numerical optimization, (accelerated gradient descent etc.), a big component of deep learning nowadays (also in ESLR).</p>", "created": "2019-01-07T08:48:40Z", "subject": "", "uid": "hyw7jbf3qk2160"}, {"anon": "no", "content": "<p>ESLR has some really frontier topics at the end, and to be fairly honest to understand probabilistic graphical models and the junction tree algorithm etc. ( I believe chapter 11), CS 281A (statistical learning theory) would be most relevant since usually the focus of this class is PGMs which generalize the Viterbi algorithm from 126. Disclaimer: I haven&#39;t taken 281A, just read a few of the notes. For the high dimensional statistics /concentration of measure material (chapter 12 I think) , this requires a high degree of mathematical maturity and knowledge of measure theoretic ideas like Radon-Nikodym derivatives. This is something that isn&#39;t really needed unless you plan on pursuing a Phd in ML, but you&#39;ll see some of the results come up in models seen often such as Lasso regression. The way to get yourself to understand this frontier topic would be to do something like take Math 104(real analysis), then 202A (measure theory) since this requires 104 level knowledge , and Stat 210A (mainly about parametric statistical models) followed by 210B (the high dimensional statistics/nonparametric statistics class here). I have not taken 210B, but have taken Math 104, 202A, and 210A, and I can answer any general questions about the material in these classes. The hypothesis testing/MLE in 210A may be a bit redundant with 126. Also , this is getting a bit far off but I believe Professor Michael Jordan (teaching 210B in Spring) recommends eventually studying functional analysis (Math 202B, although it was introduced at the end of our 202A class) for nonparametric statistics if you plan on doing really theoretical ML work. Personally I&#39;ve decided to put off 210B and 202B till graduate school but again this kind of depends on what area of statistical learning you want to study from ESLR. Also, it may seem slightly strange at first, but EE229A if you have sufficient preparation (sounds like you definitely do with 126) does cover concentration of measure material at the end from an information theoretic perspective. I have also taken this, along with Math 110, so feel free to ask about it as well. My opinion of Math 110 is that it&#39;s very useful for looking at linear algebra from an axiomatic point of view, and really understanding positive definite, semi definite forms etc. will help in the quadratic programming part of 127. For numerical linear algebra that&#39;s used in optimization though it does not cover much in terms of SVD etc. Math 221 or 128A covers this better (I only took the latter). Numerical analysis also introduces you to proving convergence of iterative techniques which is really useful for numerical optimization, which is a big part of deep learning nowadays (also in ESLR).</p>", "created": "2019-01-07T08:46:33Z", "subject": "", "uid": "hyw7jbf3qk2160"}, {"anon": "no", "content": "<p>ESLR has some really frontier topics at the end, and to be fairly honest to understand probabilistic graphical models and the junction tree algorithm etc. ( I believe chapter 11), CS 281A (statistical learning theory) would be most relevant since usually the focus of this class is PGMs which generalize the Viterbi algorithm from 126. Disclaimer: I haven&#39;t taken 281A, just read a few of the notes. For the high dimensional statistics /concentration of measure material (chapter 12 I think) , this requires a high degree of mathematical maturity and knowledge of measure theoretic ideas like Radon-Nikodym derivatives. This is something that isn&#39;t really needed unless you plan on pursuing a Phd in ML, but you&#39;ll see some of the results come up in models seen often such as Lasso regression. The way to get yourself to understand this frontier topic would be to do something like take Math 104(real analysis), then 202A (measure theory) since this requires 104 level knowledge , and Stat 210A (mainly about parametric statistical models) followed by 210B (the high dimensional statistics/nonparametric statistics class here). I have not taken 210B, but have taken Math 104, 202A, and 210A, and I can answer any general questions about the material in these classes. The hypothesis testing/MLE in 210A may be a bit redundant with 126. Also , this is getting a bit far off but I believe Professor Michael Jordan (teaching 210B in Spring) recommends eventually studying functional analysis (Math 202B, although it was introduced at the end of our 202A class) for nonparametric statistics if you plan on doing really theoretical ML work. Personally I&#39;ve decided to put off 210B and 202B till graduate school but again this kind of depends on what area of statistical learning you want to study from ESLR. Also, it may seem slightly strange at first, but EE229A if you have sufficient preparation (sounds like you definitely do with 126) does cover concentration of measure material at the end from an information theoretic perspective. I have also taken this, along with Math 110, so feel free to ask about it as well. My opinion of Math 110 is that it&#39;s very useful for looking at linear algebra from an axiomatic point of view, and really understanding positive definite, semi definite forms etc. will help in the quadratic programming part of 127. For numerical linear algebra that&#39;s used in optimization though it does not cover much in terms of SVD etc. Math 221 or 128A covers this better (I only took the latter). Numerical analysis also introduces you to proving convergence of iterative techniques which is really useful for numerical optimization, which is a big part of statistical learning nowadays.</p>", "created": "2019-01-07T08:45:12Z", "subject": "", "uid": "hyw7jbf3qk2160"}], "id": "jqm2z4oxwqp2tw", "is_tag_endorse": false, "tag_endorse": [{"admin": false, "endorser": {"global": 1557212965, "jpdq1epqrr3231": 1557211704, "jqtyr3q4ryo4z3": 1554878989}, "facebook_id": null, "id": "j3qb8b5k8gni8", "name": "Bryan Chen", "photo": "1548706388_35.png", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/j3qb8b5k8gni8/1548706388_35.png", "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {"global": 1525153666, "hyq0br1u3kx7dg": 1, "j7s01y165odq5": 1527030308, "j9ladzzp4wmh": 1525109355, "jkieu4nv1t55g8": 1539234857}, "facebook_id": null, "id": "is6p9v8jj8x141", "name": "Mohammed Shaikh", "photo": "1515230849_35.png", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/is6p9v8jj8x141/1515230849_35.png", "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "is6p6j9n6u55fp", "name": "Katie Gu", "photo": "1513749563_35.png", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/is6p6j9n6u55fp/1513749563_35.png", "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {"jpslgn3m4lsk6": 1555223627}, "facebook_id": null, "id": "jhobhold1yokb", "name": "Kaushik Sankar", "photo": null, "photo_url": null, "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "is6pa2e9c481b9", "name": "Ashwinee Panda", "photo": null, "photo_url": null, "published": true, "role": "student", "us": false}, {"admin": false, "class_sections": ["classof2019_hyq0br1u3kx7dg"], "endorser": {"jjnjp9mea54uy": 1536696387}, "facebook_id": null, "id": "icguy6ln9pj1pi", "name": "Srinivasa Pranav", "photo": null, "photo_url": null, "published": true, "role": "", "us": false}], "tag_endorse_arr": ["j3qb8b5k8gni8", "is6p9v8jj8x141", "is6p6j9n6u55fp", "jhobhold1yokb", "is6pa2e9c481b9", "icguy6ln9pj1pi"], "type": "s_answer"}, {"anon": "stud", "bucket_name": "Week 1/6 - 1/12", "bucket_order": 63, "children": [{"anon": "no", "bucket_name": "Week 1/6 - 1/12", "bucket_order": 63, "children": [], "config": {}, "created": "2019-01-07T09:10:05Z", "data": {"embed_links": null}, "folders": [], "id": "jqm3v4l8ytc56z", "subject": "<p>No problem, and I think you will learn most of 110 (other than the Jordan Canonical form) from 54 which you&#39;ve taken already and 127, so no not more worth it.</p>", "tag_good": [], "tag_good_arr": [], "type": "feedback", "uid": "hyw7jbf3qk2160", "updated": "2019-01-07T09:10:05Z"}, {"anon": "stud", "bucket_name": "Week 1/6 - 1/12", "bucket_order": 63, "children": [], "config": {}, "created": "2019-01-07T09:17:07Z", "data": {"embed_links": null}, "folders": [], "id": "jqm446f4khvej", "subject": "<p>Sounds good. Do you think 127/104/189 is doable? How would it compare in workload to 170/126/breadth?</p>", "tag_good": [], "tag_good_arr": [], "type": "feedback", "uid_a": "a_0", "updated": "2019-01-07T09:17:07Z"}, {"anon": "no", "bucket_name": "Week 1/6 - 1/12", "bucket_order": 63, "children": [], "config": {}, "created": "2019-01-07T09:48:51Z", "data": {"embed_links": null}, "folders": [], "id": "jqm58znet3b16b", "subject": "<p>in my opinion, Math104 was less work than CS170. But in terms of difficulty of the material itself, just as hard. There definitely is some overlap between 127 and 189, especially how regularization is explained in 127, so I would say the first option is doable</p>", "tag_good": [], "tag_good_arr": [], "type": "feedback", "uid": "hyw7jbf3qk2160", "updated": "2019-01-07T09:48:51Z"}, {"anon": "stud", "bucket_name": "Week 3/10 - 3/16", "bucket_order": 54, "children": [], "config": {}, "created": "2019-03-13T19:58:32Z", "data": {"embed_links": null}, "folders": [], "id": "jt7mnew8d8q2ct", "subject": "<p>Hey Luke, thanks for your advice from before. Not sure if you&#39;ll see this from a piazza notification, but I was wondering if you had recommendations for the next math class after 104? I&#39;m planning on taking ee227 &#43; 61c &#43; breadth &#43; a math class next semester.</p>", "tag_good": [], "tag_good_arr": [], "type": "feedback", "uid_a": "a_0", "updated": "2019-03-13T19:58:32Z"}], "config": {}, "created": "2019-01-07T09:07:14Z", "data": {"embed_links": null}, "folders": [], "id": "jqm3rgfmwnn3sa", "no_answer": 0, "no_upvotes": 0, "subject": "<p>&#64;luke thank you for the comprehensive response! very much appreciated.\u00a0</p>\n<p>Given my background, what classes would you recommend I take? I am in 127/188/189 right now, but do you think math 110 is more worth it than 127? or should i take both, if so, which one first (with 189 at the same time)?</p>", "tag_good": [], "tag_good_arr": [], "type": "followup", "uid_a": "a_0", "updated": "2019-01-07T09:07:14Z"}], "config": {}, "created": "2019-01-03T11:52:26Z", "data": {"embed_links": []}, "default_anonymity": "no", "drafts": null, "folders": ["lscs"], "history": [{"anon": "stud", "content": "<p>background: taken ds100, ee126, cs170 and done well; math 53/54 at cc</p>\n<p></p>\n<p>if i wanted to eventually understand\u00a0<a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf\">ESL</a>, what sequence of courses could get me there? using 126 knowledge I\u00a0<em>guess\u00a0</em>I could work through it but it feels like I need some more foundation, looking for someone who&#39;s read the book before for advice.</p>\n<p></p>\n<p>I&#39;m currently enrolled in 188/189 for next semester, so recommending a course I could tack on to that for next semester would be great as well. (127 is the current candidate, but I&#39;ve heard that it&#39;s not the greatest.) Maybe math 110,\u00a0 104? Some statistics classes I have no idea about?\u00a0 Thanks!</p>\n<p></p>\n<p></p>\n<p></p>", "created": "2019-01-03T11:52:26Z", "subject": "Courses to eventually understand Elements of Statistical Learning?", "uid_a": "a_0"}], "i_edits": [], "id": "jqgjwhrzd6r1ty", "is_bookmarked": false, "is_tag_good": false, "my_favorite": false, "no_answer": 0, "no_answer_followup": 0, "nr": 8510, "num_favorites": 9, "q_edits": [], "request_instructor": 0, "request_instructor_me": false, "s_edits": [], "status": "active", "t": 1654547703831, "tag_good": [{"admin": false, "endorser": {}, "facebook_id": null, "id": "jktvex6279x3zb", "name": "Maxwell Chen", "photo": "1579165146_200.jpg", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/jktvex6279x3zb/1579165146_200.jpg", "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "idrilg16trt4ij", "name": "Matthew Joerke", "photo": null, "photo_url": null, "published": true, "role": "student", "us": false}], "tag_good_arr": ["jktvex6279x3zb", "idrilg16trt4ij"], "tags": ["lscs", "student"], "type": "question", "unique_views": 799}