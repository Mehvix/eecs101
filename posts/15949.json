{"bookmarked": 6, "bucket_name": "Today", "bucket_order": 3, "change_log": [{"anon": "stud", "data": "kv0huxwrwxv64f", "type": "create", "uid_a": "a_0", "v": "all", "when": "2021-10-21T05:18:51Z"}, {"anon": "stud", "data": "kv0iklrlm1h4j9", "to": "kv0huxwn47q64e", "type": "s_answer", "uid_a": "a_1", "when": "2021-10-21T05:38:48Z"}, {"anon": "stud", "data": "kv0il5em5f5682", "type": "s_answer_update", "uid_a": "a_1", "when": "2021-10-21T05:39:14Z"}], "children": [{"bucket_name": "Today", "bucket_order": 3, "children": [], "config": {"editor": "rte"}, "created": "2021-10-21T05:38:48Z", "data": {"embed_links": []}, "folders": [], "history": [{"anon": "stud", "content": "<p>It depends on the type of model you are using I believe. Many forward pass methods have an optional field called something like mems or past_key_values that you can set to a list of hidden state vectors for the tokens you don&#39;t want recomputed. These tokens should not be included in the set you pass in for input_ids.</p>", "created": "2021-10-21T05:39:14Z", "subject": "", "uid_a": "a_1"}, {"anon": "stud", "content": "<p>It depends on the type of model you are using I believe. Many forward pass methods have an optional field called something like mems or past_key_values that you can set to a list of hidden state vectors for the tokens you don&#39;t want recomputed. These tokens should not be included in the set you pass for input_ids.</p>", "created": "2021-10-21T05:38:48Z", "subject": "", "uid_a": "a_1"}], "history_size": 2, "id": "kv0iklrbnoo4j8", "is_tag_endorse": false, "tag_endorse": [], "tag_endorse_arr": [], "type": "s_answer"}], "config": {"editor": "rte", "has_emails_sent": 1, "seen": {"10342": 2, "11212": 5, "11584": 6, "12404": 4, "14124": 1, "14610": 7, "15555": 0, "2815": 3, "9764": 8, "9832": 9}}, "created": "2021-10-21T05:18:51Z", "data": {"embed_links": []}, "default_anonymity": "no", "drafts": null, "folders": ["other"], "history": [{"anon": "stud", "content": "<p>From https://github.com/huggingface/transformers/blob/5c3b32d44d0164aaa9b91405f48e53cf53a82b35/examples/run_generation.py#L124,</p>\n<pre>\noutputs = model(**inputs) \u00a0# Note: we could also use &#39;past&#39; with GPT-2/Transfo-XL/XLNet (cached hidden-states) </pre>\n<p>May I ask if anyone know how to use cached hidden-states so that the model doesn&#39;t need to perform every single forward pass? Instead, the model can use the previously cached tokens to later text generation to speed up model inference?</p>\n<p></p>", "created": "2021-10-21T05:18:51Z", "subject": "How to use cached hidden states in run_generation ?", "uid_a": "a_0"}], "history_size": 1, "i_edits": [], "id": "kv0huxwn47q64e", "is_bookmarked": false, "is_tag_good": false, "my_favorite": false, "no_answer": 0, "no_answer_followup": 0, "nr": 15949, "num_favorites": 0, "q_edits": [], "request_instructor": 0, "request_instructor_me": false, "s_edits": [], "status": "active", "t": 1654539451432, "tag_good": [], "tag_good_arr": [], "tags": ["other", "student"], "type": "question", "unique_views": 270}