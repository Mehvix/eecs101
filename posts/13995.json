{"bookmarked": 6, "bucket_name": "Today", "bucket_order": 3, "change_log": [{"anon": "stud", "data": "kj3phlae1dc2ut", "type": "create", "uid_a": "a_0", "v": "all", "when": "2020-12-25T03:19:15Z"}, {"anon": "stud", "data": "kj3phppwgep2yt", "type": "update", "uid_a": "a_0", "when": "2020-12-25T03:19:21Z"}], "children": [], "config": {"seen": {"3644": 0, "4667": 5, "5197": 2, "5290": 6, "5500": 1, "6857": 4, "7348": 7, "7574": 8, "7772": 3, "8916": 9}}, "created": "2020-12-25T03:19:15Z", "data": {"embed_links": []}, "default_anonymity": "no", "drafts": null, "folders": ["other", "enrollments", "eecs", "cs-intended", "events", "lscs", "research", "faculty_advising", "policy"], "history": [{"anon": "stud", "content": "<p>Professor Sahai&#39;s advice on CS 189 (and what prerequisites are needed):</p>\n<p>&#34;&#34;&#34;</p>\n<p>In an ideal dream world, we&#39;d be teaching you the standard foundational ML course from the year 2025 today. Because that&#39;s about the sweet spot for when the peak (comparative) impact of what we teach you is going to be. Early on, it doesn&#39;t matter that much what exactly we teach you because you&#39;ll be getting large gains from practice. It&#39;s after those initial gains that the long-term impact of a course starts to show --- we want to give you the solid grounding that will be able to help you navigate even as the field evolves and (seemingly) new things emerge. Of course, we don&#39;t have a time machine so we&#39;ll have to just do our best to approximate this &#34;Basic ML from 2025&#34; using our judgement.</p>\n<p></p>\n<p>It&#39;s also good to keep in mind what this course is not. It is not a course in linear algebra. It is not a course in how to program. Nor is it a probability course or an optimization course. We&#39;re going to use all those things, and along the way you&#39;ll probably deepen your understanding of them from our use and perspective, but we&#39;re not trying to take the place of courses like 127/227A or 126, etc. Certainly we&#39;re not trying to substitute for the lower-division basics! Probability and Optimization are broadly useful topics and even if the current buzzwords out there might, for marketing reasons, refer to many of those applications all under the buzzwords of &#34;ML&#34; or &#34;AI&#34;, this course has a reasonably narrow focus by comparison. Especially if you&#39;re thinking of applying ML thinking to new domains, you&#39;re almost certainly going to need more optimization thinking and/or probability concepts to really achieve what you want.</p>\n<p></p>\n<p>The philosophy of this course can best be explained using the analogy of cooking. Some people (A) think about cooking as the ability to follow recipes well. They want to be able to read a recipe and be able to make the dish that is described. Maybe going a bit beyond that, they want to be able to adjust recipes to the size of the dinner party that they are going to be throwing. Going a bit further, they might want to be able to do some substitutions if certain ingredients are missing. But another group of people (B) think about cooking differently. They want the ability to look in the pantry and the garden, see what they&#39;ve got available, and be able to make something to eat for a given occasion. Going beyond that, they think that knowing cooking is being able to enjoy a dish at a restaurant, and then take their appreciation of those flavors and ingredients, and leverage that when they make something on their own. This course is decidedly trying to get you started on road (B) vis-a-vis machine learning. Recipes as inspiration, not dogma.</p>\n<p></p>\n<p>Basically from our point of view, if all you learn from this course is a collection of machine learning techniques and what kinds of problems each is appropriate for, that is a &#34;C&#34; level of understanding. You can pass the class, but that&#39;s not good enough for us to call that a good understanding. What we want you to understand are the underlying ideas, principles, and perspectives --- so that you can easily pick up and adapt machine learning techniques as your career advances, and weave the ideas into whatever it is that you want to do as appropriate. We want you to appreciate the new insights and approaches that are going to be developed during your career, but we&#39;d rather you not be bowled over by every bit of clever marketing and hype out there. We&#39;d rather you see the points of commonality with what you know, so that you can better appreciate the actual new contributions being made.</p>\n<p></p>\n<p>To help you get started on that journey, we&#39;re going to be taking a steady and carefully curated path. New ideas will keep coming, and the course is largely cumulative. We are going to try and engage with concepts in their simplest forms first, so that you can hopefully get familiar with the core ideas. Pedagogically, we&#39;ll favor something simple over something that is more effective in practice --- especially at the beginning. And we&#39;re going to give you weekly homework that will make you actually engage with the ideas. Often times, we will introduce new ideas in the homework if that&#39;s the most effective way to do it. We&#39;re also going to build on all the basic ideas that you&#39;ve already seen in earlier courses: you&#39;re going to see polynomials, Fourier concepts, interpolation ideas, projections and inner products, SVDs, OMP, the &#34;natural coordinates&#34; point of view, Gaussian random variables, random variables as an inner-product space, etc. as well as everything that you&#39;ve already seen (minimum norms, iterative approaches, etc.) reviewed in HW0.\u00a0</p>\n<p></p>\n<p>The reason is that we would ideally like for you to learn how to build concepts on a solid foundation in which you can back them up with at least three prongs of support in the form of paradigmatic examples or stories. (1) A story that is theoretical and has results that you can derive analytically; (2) A computational story that you can visualize and explore that is based in synthetic data --- the counterpart of a laboratory experiment; (3) A computational story that involves a (possibly stylized) real-world data set. In other words, we are going to be treating machine learning the same way that you&#39;ve learned everything else. This course does not have a physical laboratory component and so for the undergraduates, your experience in EECS16B with the robot car will remain the most truly realistic machine learning that you will experience in the classroom. But hopefully, you will be able to bring some of the feeling and skepticism from that experience to the more stylized examples we will make you engage with here. (Graduate students from other areas should bring their standard experimental experiences with data with them.)</p>\n<p></p>\n<p>Theoretical models for us are going to be useful toys to play with so you can understand how and why things work. There are going to be gaps between what theorems say and what you are going to explore in simulation and numerical experiments. Some of this reflects the limited state of our understanding at the present --- in some cases, the theoretical frameworks we have give conjectures but we don&#39;t have proofs yet. In other cases, we don&#39;t even have what could properly be called conjectures yet. Instead, what we have are somewhat vague intuitions and qualitative guesses. And in other cases, we don&#39;t even have that. All we have is a feeling that this rhymes with that, but we don&#39;t even know what rhyming means exactly.\u00a0</p>\n<p></p>\n<p>It is for this reason that our course will also be trying to expose you to some key principles of how to be honest and careful while doing empirical investigation of learning. Remember, human beings were building grand cathedrals before they properly understood F=MA or the basics of Newtonian mechanics. Chemistry was working with valences before Physics had quantum mechanics. Mysteries and gaps in our understanding aren&#39;t unusual --- they are and have been the norm for most of human civilization. Engineers have to navigate them because stuff needs to be done even when we don&#39;t really understand what we are doing. A willingness to experiment and try new things, even if they might feel a bit crazy at times, is important for real world progress. Machine learning has certainly benefited greatly from this YOLO spirit as a field. At the same time, you have to be disciplined and sincere to avoid self-delusion.</p>\n<p></p>\n<p>Of course, you&#39;re also going to have to learn techniques. After we&#39;ve built a basic foundation for you, we&#39;re going to be exposing you to many techniques. However, the course is decidedly not a &#34;one thing after another&#34; kind of course. In the homeworks and exams, you&#39;ll be expected to be able to connect the core principles and ideas to diverse seeming techniques.\u00a0</p>\n<p></p>\n<p>There will be another post about how to succeed, but hopefully you can tell from this post what the most important thing is going to be: you have to keep up and not fall behind. Put in the work, and you will be fine. But there will be a lot of work.\u00a0</p>\n<p>&#34;&#34;&#34;</p>", "created": "2020-12-25T03:19:21Z", "subject": "CS 189 Advice", "uid_a": "a_0"}, {"anon": "stud", "content": "<p>Professor Sahai&#39;s advice on CS 189 (and what prerequisites are needed):</p>\n<p>&#34;&#34;&#34;</p>\n<p>In an ideal dream world, we&#39;d be teaching you the standard foundational ML course from the year 2025 today. Because that&#39;s about the sweet spot for when the peak (comparative) impact of what we teach you is going to be. Early on, it doesn&#39;t matter that much what exactly we teach you because you&#39;ll be getting large gains from practice. It&#39;s after those initial gains that the long-term impact of a course starts to show --- we want to give you the solid grounding that will be able to help you navigate even as the field evolves and (seemingly) new things emerge. Of course, we don&#39;t have a time machine so we&#39;ll have to just do our best to approximate this &#34;Basic ML from 2025&#34; using our judgement.</p>\n<p></p>\n<p>It&#39;s also good to keep in mind what this course is not. It is not a course in linear algebra. It is not a course in how to program. Nor is it a probability course or an optimization course. We&#39;re going to use all those things, and along the way you&#39;ll probably deepen your understanding of them from our use and perspective, but we&#39;re not trying to take the place of courses like 127/227A or 126, etc. Certainly we&#39;re not trying to substitute for the lower-division basics! Probability and Optimization are broadly useful topics and even if the current buzzwords out there might, for marketing reasons, refer to many of those applications all under the buzzwords of &#34;ML&#34; or &#34;AI&#34;, this course has a reasonably narrow focus by comparison. Especially if you&#39;re thinking of applying ML thinking to new domains, you&#39;re almost certainly going to need more optimization thinking and/or probability concepts to really achieve what you want.</p>\n<p></p>\n<p>The philosophy of this course can best be explained using the analogy of cooking. Some people (A) think about cooking as the ability to follow recipes well. They want to be able to read a recipe and be able to make the dish that is described. Maybe going a bit beyond that, they want to be able to adjust recipes to the size of the dinner party that they are going to be throwing. Going a bit further, they might want to be able to do some substitutions if certain ingredients are missing. But another group of people (B) think about cooking differently. They want the ability to look in the pantry and the garden, see what they&#39;ve got available, and be able to make something to eat for a given occasion. Going beyond that, they think that knowing cooking is being able to enjoy a dish at a restaurant, and then take their appreciation of those flavors and ingredients, and leverage that when they make something on their own. This course is decidedly trying to get you started on road (B) vis-a-vis machine learning. Recipes as inspiration, not dogma.</p>\n<p></p>\n<p>Basically from our point of view, if all you learn from this course is a collection of machine learning techniques and what kinds of problems each is appropriate for, that is a &#34;C&#34; level of understanding. You can pass the class, but that&#39;s not good enough for us to call that a good understanding. What we want you to understand are the underlying ideas, principles, and perspectives --- so that you can easily pick up and adapt machine learning techniques as your career advances, and weave the ideas into whatever it is that you want to do as appropriate. We want you to appreciate the new insights and approaches that are going to be developed during your career, but we&#39;d rather you not be bowled over by every bit of clever marketing and hype out there. We&#39;d rather you see the points of commonality with what you know, so that you can better appreciate the actual new contributions being made.</p>\n<p></p>\n<p>To help you get started on that journey, we&#39;re going to be taking a steady and carefully curated path. New ideas will keep coming, and the course is largely cumulative. We are going to try and engage with concepts in their simplest forms first, so that you can hopefully get familiar with the core ideas. Pedagogically, we&#39;ll favor something simple over something that is more effective in practice --- especially at the beginning. And we&#39;re going to give you weekly homework that will make you actually engage with the ideas. Often times, we will introduce new ideas in the homework if that&#39;s the most effective way to do it. We&#39;re also going to build on all the basic ideas that you&#39;ve already seen in earlier courses: you&#39;re going to see polynomials, Fourier concepts, interpolation ideas, projections and inner products, SVDs, OMP, the &#34;natural coordinates&#34; point of view, Gaussian random variables, random variables as an inner-product space, etc. as well as everything that you&#39;ve already seen (minimum norms, iterative approaches, etc.) reviewed in HW0.\u00a0</p>\n<p></p>\n<p>The reason is that we would ideally like for you to learn how to build concepts on a solid foundation in which you can back them up with at least three prongs of support in the form of paradigmatic examples or stories. (1) A story that is theoretical and has results that you can derive analytically; (2) A computational story that you can visualize and explore that is based in synthetic data --- the counterpart of a laboratory experiment; (3) A computational story that involves a (possibly stylized) real-world data set. In other words, we are going to be treating machine learning the same way that you&#39;ve learned everything else. This course does not have a physical laboratory component and so for the undergraduates, your experience in EECS16B with the robot car will remain the most truly realistic machine learning that you will experience in the classroom. But hopefully, you will be able to bring some of the feeling and skepticism from that experience to the more stylized examples we will make you engage with here. (Graduate students from other areas should bring their standard experimental experiences with data with them.)</p>\n<p></p>\n<p>Theoretical models for us are going to be useful toys to play with so you can understand how and why things work. There are going to be gaps between what theorems say and what you are going to explore in simulation and numerical experiments. Some of this reflects the limited state of our understanding at the present --- in some cases, the theoretical frameworks we have give conjectures but we don&#39;t have proofs yet. In other cases, we don&#39;t even have what could properly be called conjectures yet. Instead, what we have are somewhat vague intuitions and qualitative guesses. And in other cases, we don&#39;t even have that. All we have is a feeling that this rhymes with that, but we don&#39;t even know what rhyming means exactly.\u00a0</p>\n<p></p>\n<p>It is for this reason that our course will also be trying to expose you to some key principles of how to be honest and careful while doing empirical investigation of learning. Remember, human beings were building grand cathedrals before they properly understood F=MA or the basics of Newtonian mechanics. Chemistry was working with valences before Physics had quantum mechanics. Mysteries and gaps in our understanding aren&#39;t unusual --- they are and have been the norm for most of human civilization. Engineers have to navigate them because stuff needs to be done even when we don&#39;t really understand what we are doing. A willingness to experiment and try new things, even if they might feel a bit crazy at times, is important for real world progress. Machine learning has certainly benefited greatly from this YOLO spirit as a field. At the same time, you have to be disciplined and sincere to avoid self-delusion.</p>\n<p></p>\n<p>Of course, you&#39;re also going to have to learn techniques. After we&#39;ve built a basic foundation for you, we&#39;re going to be exposing you to many techniques. However, the course is decidedly not a &#34;one thing after another&#34; kind of course. In the homeworks and exams, you&#39;ll be expected to be able to connect the core principles and ideas to diverse seeming techniques.\u00a0</p>\n<p></p>\n<p>There will be another post about how to succeed, but hopefully you can tell from this post what the most important thing is going to be: you have to keep up and not fall behind. Put in the work, and you will be fine. But there will be a lot of work.\u00a0</p>\n<p>&#34;&#34;&#34;</p>", "created": "2020-12-25T03:19:15Z", "subject": "CS 189 Advice", "uid_a": "a_0"}], "history_size": 2, "i_edits": [], "id": "kj3phlabvvw2us", "is_bookmarked": false, "is_tag_good": false, "my_favorite": false, "no_answer_followup": 0, "nr": 13995, "num_favorites": 3, "q_edits": [], "request_instructor": 0, "request_instructor_me": false, "s_edits": [], "status": "active", "t": 1654541608755, "tag_good": [{"admin": false, "endorser": {}, "facebook_id": null, "id": "jyeobq4n3ao7kw", "name": "Merrick Wang", "photo": "1584281693_200.jpg", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/jyeobq4n3ao7kw/1584281693_200.jpg", "published": true, "role": "", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "jzdh1khvstn33f", "name": "Roopak Phatak", "photo": null, "photo_url": null, "role": "student", "us": false}, {"admin": false, "endorser": {"kdz4wzqnb6052o": 1603740009}, "facebook_id": null, "id": "kdzjqzylq2a5gx", "name": "Roshan Regula", "photo": null, "photo_url": null, "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {"jzllv02dp7j3jh": 1571985065, "kdhvjzdceb91ll": 1607826884}, "facebook_id": null, "id": "jktvg5rtq4e4uz", "name": "Jay Monga", "photo": "1580804937_200.jpg", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/jktvg5rtq4e4uz/1580804937_200.jpg", "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "jktvhcjpozd5xg", "name": "Atharva Mehendale", "photo": "1596323170_200.jpg", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/jktvhcjpozd5xg/1596323170_200.jpg", "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "kbjuv006k3w6ya", "name": "Artem Tkachuk", "photo": "1598657496_200.jpg", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/kbjuv006k3w6ya/1598657496_200.jpg", "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {"jyq487ctcwuk8": 1570760427}, "facebook_id": null, "id": "jzccssa5qo462l", "name": "Hari Vallabhaneni", "photo": "1580344168_200.jpg", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/jzccssa5qo462l/1580344168_200.jpg", "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "j6lrx7lxeqz3e4", "name": "Tsai-Jwu (Judy)", "photo": null, "photo_url": null, "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "jzlnpfrl2qk2e6", "name": "Justin Xia", "photo": null, "photo_url": null, "published": true, "role": "student", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "k5k3fsp6g1u6lv", "name": "Dayne Tran", "photo": "1595720343_200.jpg", "photo_url": "https://d1b10bmlvqabco.cloudfront.net/photos/k5k3fsp6g1u6lv/1595720343_200.jpg", "published": true, "role": "student", "us": false}], "tag_good_arr": ["jyeobq4n3ao7kw", "jzdh1khvstn33f", "kdzjqzylq2a5gx", "jktvg5rtq4e4uz", "jktvhcjpozd5xg", "kbjuv006k3w6ya", "jzccssa5qo462l", "j6lrx7lxeqz3e4", "jzlnpfrl2qk2e6", "k5k3fsp6g1u6lv"], "tags": ["cs-intended", "eecs", "enrollments", "events", "faculty_advising", "lscs", "other", "policy", "research", "student", "unanswered"], "type": "note", "unique_views": 486}