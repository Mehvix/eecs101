{"bookmarked": 8, "bucket_name": "Today", "bucket_order": 3, "change_log": [{"anon": "stud", "data": "l2fkrq20ysa3cj", "type": "create", "uid_a": "a_0", "v": "all", "when": "2022-04-26T03:14:49Z"}, {"anon": "stud", "data": "l2fksq82vmg6nb", "type": "update", "uid_a": "a_0", "when": "2022-04-26T03:15:36Z"}], "children": [], "config": {"editor": "rte", "has_emails_sent": 1, "seen": {"14369": 3, "15737": 7, "16326": 8, "16370": 4, "16690": 0, "16703": 1, "5510": 6, "6002": 5, "6512": 9, "8521": 2}}, "created": "2022-04-26T03:14:49Z", "data": {"embed_links": []}, "default_anonymity": "no", "drafts": null, "folders": ["enrichment_opportunities", "peer_directed"], "history": [{"anon": "stud", "content": "<p><a href=\"https://eaberkeley.com/\">Effective Altruism at Berkeley</a> is organizing a second round of our Artificial Intelligence Misalignment Solutions (AIMS) contest series and we want to encourage university students to participate in the Distillation Contest, with prizes as high as $2,500!</p>\n<p></p>\n<p>In the current, second round of this contest, you can directly support the <a href=\"https://intelligence.org/why-ai-safety/\">AI Safety/Alignment</a> community by creating valuable distillations of complex alignment research. <a href=\"https://www.alignmentforum.org/tag/distillation-and-pedagogy\">Distillation</a> is the practice of \u201ctaking a complex subject and making it easier to understand.\u201d Distillation is more involved than summarizing; a distillation also communicates new examples and applications of the original research.</p>\n<p></p>\n<p>The AIMS contest series was created in order to provide UC Berkeley students with a tangible opportunity to engage with the <a href=\"https://bdtechtalks.com/2021/01/18/ai-alignment-problem-brian-christian/\">Alignment Problem</a>. The field of AI Alignment/AI Safety <a href=\"https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers\">needs more distillers</a> to improve communication <a href=\"https://distill.pub/2017/research-debt/\">within the field</a>, as well as to make their research accessible to a wider audience. Although participants will need to do technical readings, this contest is a test of communication skills, so prior experience is not required; we\u2019ve provided resources on our website and would be glad to talk to you about your uncertainties.</p>\n<p></p>\n<p>This contest opened on our <a href=\"https://eaberkeley.com/aims-distillation\">website</a>, and submissions are due on May 20th! We are offering a $2500 first place prize, a $1250 second place prize, five $500 prizes, and ten $250 prizes. Fill out our <a href=\"https://airtable.com/shrUmIqmAoLaNlBQV\">interest form</a> if you are interested in receiving updates on deadlines, potential workshops, and resources for the Distillation contest.</p>\n<p></p>\n<p>If you have any questions, feel free to email them to <a href=\"mailto:admin@eaberkeley.com\">admin@eaberkeley.com</a> or schedule a 15 minute call with one of our organizers on the contest website.<br /><a href=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkdzjqc0j7uz51m%2F21a2c038cdd7932c3f621f1fa7a9a43beb0b5d05ee9df15e85dfa942213b95fe%2FAIMS_Distillation_Contest.png\" target=\"_blank\" rel=\"noopener noreferrer\"></a></p>\n<p></p>\n<p><a href=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkdzjqc0j7uz51m%2F21a2c038cdd7932c3f621f1fa7a9a43beb0b5d05ee9df15e85dfa942213b95fe%2FAIMS_Distillation_Contest.png\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkdzjqc0j7uz51m%2F21a2c038cdd7932c3f621f1fa7a9a43beb0b5d05ee9df15e85dfa942213b95fe%2FAIMS_Distillation_Contest.png\" width=\"511\" height=\"681\" alt=\"\" /></a></p>\n<p></p>", "created": "2022-04-26T03:15:36Z", "subject": "Second Contest in the AI Alignment Series!", "uid_a": "a_0"}, {"anon": "stud", "content": "<md>[Effective Altruism at Berkeley](https://eaberkeley.com/) is organizing a second round of our Artificial Intelligence Misalignment Solutions (AIMS) contest series and we want to encourage university students to participate in the Distillation Contest, with prizes as high as $2,500!\n\nIn the current, second round of this contest, you can directly support the [AI Safety/Alignment](https://intelligence.org/why-ai-safety/) community by creating valuable distillations of complex alignment research. [Distillation](https://www.alignmentforum.org/tag/distillation-and-pedagogy) is the practice of \u201ctaking a complex subject and making it easier to understand.\u201d Distillation is more involved than summarizing; a distillation also communicates new examples and applications of the original research.\u00a0\n\n\n\nThe AIMS contest series was created in order to provide UC Berkeley students with a tangible opportunity to engage with the [Alignment Problem](https://bdtechtalks.com/2021/01/18/ai-alignment-problem-brian-christian/). The field of AI Alignment/AI Safety [needs more distillers](https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers) to improve communication [within the field](https://distill.pub/2017/research-debt/), as well as to make their research accessible to a wider audience. Although participants will need to do technical readings, this contest is a test of communication skills, so prior experience is not required; we\u2019ve provided resources on our website and would be glad to talk to you about your uncertainties.\n\n\n\nThis contest opened on our [website](https://eaberkeley.com/aims-distillation), and submissions are due on May 20th! We are offering a $2500 first place prize, a $1250 second place prize, five $500 prizes, and ten $250 prizes. Fill out our [interest form](https://airtable.com/shrUmIqmAoLaNlBQV) if you are interested in receiving updates on deadlines, potential workshops, and resources for the Distillation contest.\n\n\n\nIf you have any questions, feel free to email them to [admin@eaberkeley.com](mailto:admin@eaberkeley.com) or schedule a 15 minute call with one of our organizers on the contest website.\n<a href=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkdzjqc0j7uz51m%2F21a2c038cdd7932c3f621f1fa7a9a43beb0b5d05ee9df15e85dfa942213b95fe%2FAIMS_Distillation_Contest.png\" target=\"_blank\" rel=\"noopener\"></a>\n\n<a href=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkdzjqc0j7uz51m%2F21a2c038cdd7932c3f621f1fa7a9a43beb0b5d05ee9df15e85dfa942213b95fe%2FAIMS_Distillation_Contest.png\" target=\"_blank\" rel=\"noopener\"><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fkdzjqc0j7uz51m%2F21a2c038cdd7932c3f621f1fa7a9a43beb0b5d05ee9df15e85dfa942213b95fe%2FAIMS_Distillation_Contest.png\" width=\"1434\" height=\"1912\" alt=\"\" /></a></md>", "created": "2022-04-26T03:14:49Z", "subject": "Second Contest in the AI Alignment Series!", "uid_a": "a_0"}], "history_size": 2, "i_edits": [], "id": "l2fkrq1whnn3ci", "is_bookmarked": false, "is_tag_good": false, "my_favorite": false, "no_answer_followup": 0, "nr": 16947, "num_favorites": 0, "q_edits": [], "request_instructor": 0, "request_instructor_me": false, "s_edits": [], "status": "active", "t": 1654538363717, "tag_good": [{"admin": false, "endorser": {}, "facebook_id": null, "id": "kdzjqc0j7uz51m", "name": "Carolyn Qian", "photo": null, "photo_url": null, "role": "student", "us": false}, {"admin": false, "endorser": {}, "facebook_id": null, "id": "kbjtile5y4b6op", "name": "Cristian Alamos", "photo": null, "photo_url": null, "published": true, "role": "student", "us": false}], "tag_good_arr": ["kdzjqc0j7uz51m", "kbjtile5y4b6op"], "tags": ["enrichment_opportunities", "student"], "type": "note", "unique_views": 223}